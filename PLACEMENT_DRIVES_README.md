# Placement Drives Feature - Implementation Guide\n\n## üéØ Overview\n\nThis document outlines the complete implementation of the **Placement Drives** feature for Interview.ai - a production-grade system that automatically scrapes, validates, and delivers personalized placement drive opportunities to users.\n\n## ‚ú® Key Features\n\n- **Automated Web Scraping**: Scrapes 6 major companies (Accenture, TCS, Capgemini, Wipro, Cognizant, Infosys) every 12 hours\n- **Smart Eligibility Matching**: AI-powered matching based on batch, CGPA, and branch\n- **Real-time Notifications**: In-app notifications for new eligible drives\n- **Interactive Dashboard**: Modern React dashboard with live data\n- **Clerk Integration**: Seamlessly integrated with existing Clerk authentication\n- **FREE Tier Compatible**: Works entirely on free tiers with no paid APIs\n\n## üèóÔ∏è Architecture\n\n```\nScheduler (Cron)\n‚Üì\nCompany Scrapers (Puppeteer)\n‚Üì\nData Validation & Normalization\n‚Üì\nSupabase Database (PostgreSQL)\n‚Üì\nEligibility Matching Engine\n‚Üì\nNotification Service\n‚Üì\nNext.js API Routes\n‚Üì\nReact Dashboard\n```\n\n## üìÅ Project Structure\n\n```\nInterview.ai/\n‚îú‚îÄ‚îÄ server/                          # Backend services\n‚îÇ   ‚îú‚îÄ‚îÄ config/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabaseClient.js        # Database client\n‚îÇ   ‚îú‚îÄ‚îÄ cron/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scheduler.js             # Job scheduler\n‚îÇ   ‚îú‚îÄ‚îÄ scrapers/                    # Company scrapers\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accenture.scraper.js\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tcs.scraper.js\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ capgemini.scraper.js\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wipro.scraper.js\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cognizant.scraper.js\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infosys.scraper.js\n‚îÇ   ‚îú‚îÄ‚îÄ services/                    # Business logic\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ notification.service.js\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eligibility.service.js\n‚îÇ   ‚îî‚îÄ‚îÄ app.js                       # Server entry point\n‚îú‚îÄ‚îÄ src/app/api/                     # Next.js API routes\n‚îÇ   ‚îú‚îÄ‚îÄ placement-drives/\n‚îÇ   ‚îú‚îÄ‚îÄ notifications/\n‚îÇ   ‚îî‚îÄ‚îÄ user-profile/\n‚îú‚îÄ‚îÄ src/app/(client)/dashboard/\n‚îÇ   ‚îî‚îÄ‚îÄ placement-drives/            # Main dashboard page\n‚îî‚îÄ‚îÄ src/components/\n    ‚îú‚îÄ‚îÄ sideMenu.tsx                 # Updated sidebar\n    ‚îî‚îÄ‚îÄ ui/                          # UI components\n```\n\n## üõ†Ô∏è Installation & Setup\n\n### 1. Database Setup\n\nRun the SQL schema in your Supabase dashboard:\n\n```bash\n# Copy and execute in Supabase SQL editor\ncat placement_drives_schema.sql\n```\n\n### 2. Install Dependencies\n\n```bash\n# Install new dependencies\nnpm install node-cron puppeteer date-fns\n```\n\n### 3. Environment Variables\n\n```bash\n# Add to your .env.local\nNEXT_PUBLIC_SUPABASE_URL=your_supabase_url\nNEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key\nSUPABASE_SERVICE_ROLE_KEY=your_service_role_key\n```\n\n### 4. Start Backend Services\n\n```bash\n# Start the placement drives backend\ncd server\nnode app.js\n```\n\n### 5. Start Frontend\n\n```bash\n# Start Next.js development server\nnpm run dev\n```\n\n## üìä Database Schema\n\n### Core Tables\n\n- **companies**: Target companies (Accenture, TCS, etc.)\n- **drives**: Placement drive records\n- **user_profiles**: Extended user profiles (linked via clerk_user_id)\n- **notifications**: In-app notifications\n- **drive_registrations**: User registration tracking (analytics)\n\n### Key Relationships\n\n```sql\ncompanies.id ‚Üí drives.company_id\nuser_profiles.clerk_user_id ‚Üí notifications.clerk_user_id\ndrives.id ‚Üí notifications.drive_id\n```\n\n## üï∑Ô∏è Scraping System\n\n### How It Works\n\n1. **Scheduled Execution**: Runs every 12 hours (6 AM & 6 PM IST)\n2. **Company-Specific Scrapers**: Each company has a dedicated scraper\n3. **Data Validation**: Validates required fields before storage\n4. **Duplicate Prevention**: UNIQUE constraints prevent duplicates\n5. **Respectful Scraping**: 5-second delays between requests\n\n### Adding New Companies\n\n1. Create new scraper file in `server/scrapers/`\n2. Follow the existing scraper pattern\n3. Add company to database\n4. Update scheduler to include new scraper\n\n```javascript\n// Example scraper structure\nclass NewCompanyScraper {\n  constructor() {\n    this.companyName = 'NewCompany';\n    this.searchUrls = [/* URLs to scrape */];\n  }\n  \n  async scrape() {\n    // Scraping logic\n  }\n  \n  async normalizeDrives(rawDrives) {\n    // Data normalization\n  }\n  \n  validateDrive(drive) {\n    // Validation logic\n  }\n}\n```\n\n## üéØ Eligibility System\n\n### Matching Criteria\n\n- **Batch Match**: User batch matches drive batch (40% weight)\n- **CGPA Requirement**: User CGPA >= drive minimum (35% weight)  \n- **Branch Eligibility**: User branch in drive's accepted branches (25% weight)\n\n### Eligibility Scores\n\n- **100%**: Fully eligible\n- **75-99%**: Mostly eligible\n- **50-74%**: Partially eligible\n- **<50%**: Not eligible\n\n### Smart Matching Features\n\n- **Fuzzy Branch Matching**: Handles variations like \"CSE\" vs \"Computer Science\"\n- **Year Extraction**: Extracts years from batch strings like \"2024 Batch\"\n- **Flexible Criteria**: Missing profile data defaults to eligible\n\n## üîî Notification System\n\n### When Notifications Are Created\n\n- New placement drive matches user criteria\n- User becomes eligible after profile update\n- Deadline approaching (optional future feature)\n\n### Notification Management\n\n- **Auto-cleanup**: Keeps max 50 notifications per user\n- **Read Status**: Track read/unread state\n- **Batch Operations**: Mark all as read functionality\n\n## üé® Frontend Features\n\n### Dashboard Tabs\n\n1. **Eligible Drives**: Personalized matches\n2. **All Drives**: Complete drive listing\n3. **Notifications**: Real-time alerts with unread count\n\n### Drive Cards\n\n- **Company Branding**: Logo and company name\n- **Eligibility Status**: Clear eligibility indicators\n- **Drive Details**: Type, deadline, CGPA requirements\n- **Quick Registration**: Direct links to company portals\n\n### Interactive Features\n\n- **Real-time Updates**: Live notification counts\n- **Smart Filtering**: Auto-filter by eligibility\n- **Responsive Design**: Mobile-friendly interface\n\n## üöÄ Production Deployment\n\n### Backend Deployment\n\n```bash\n# Option 1: Node.js Server\npm2 start server/app.js --name placement-drives\n\n# Option 2: Docker\ndocker build -t placement-drives-backend .\ndocker run -d --name placement-backend placement-drives-backend\n\n# Option 3: Serverless (Vercel Functions)\n# Deploy as Vercel function with cron triggers\n```\n\n### Monitoring & Logging\n\n```javascript\n// Add to your monitoring dashboard\nscheduler.getStats(); // Get scraping statistics\n\n// Example response:\n{\n  totalRuns: 45,\n  successfulRuns: 44,\n  totalDrivesScraped: 1250,\n  totalDrivesInserted: 89,\n  lastRun: \"2024-01-15T06:00:00Z\",\n  nextRun: \"2024-01-15T18:00:00Z\"\n}\n```\n\n## üõ°Ô∏è Security & Compliance\n\n### Data Privacy\n\n- Only scrapes publicly available job postings\n- No personal data collection during scraping\n- User data encrypted in Supabase\n- Row-level security policies implemented\n\n### Rate Limiting\n\n- 5-second delays between company scrapes\n- Respectful crawling practices\n- Timeout protections (30s per page)\n\n### Error Handling\n\n- Graceful failure handling\n- Automatic retry logic\n- Comprehensive error logging\n- Circuit breaker patterns\n\n## üìà Performance & Scaling\n\n### Database Optimization\n\n```sql\n-- Key indexes for performance\nCREATE INDEX idx_drives_deadline ON drives(deadline);\nCREATE INDEX idx_drives_company_active ON drives(company_id, is_active);\nCREATE INDEX idx_notifications_user_unread ON notifications(clerk_user_id, is_read);\n```\n\n### Caching Strategy\n\n- User eligibility caching\n- Drive data caching (15-minute TTL)\n- Notification count caching\n\n### Scaling Considerations\n\n- **Horizontal Scaling**: Multiple scraper instances\n- **Database Sharding**: Partition by company or date\n- **CDN Integration**: Cache static assets\n- **Queue System**: Asynchronous notification processing\n\n## üß™ Testing\n\n### Manual Testing\n\n```bash\n# Test individual scraper\nconst scraper = new AccentureScraper();\nconst drives = await scraper.scrape();\nconsole.log(`Found ${drives.length} drives`);\n\n# Test eligibility matching\nconst eligibility = eligibilityService.checkEligibility(userProfile, drive);\nconsole.log(`Eligibility: ${eligibility.isEligible} (${eligibility.score}%)`);\n```\n\n### Automated Tests\n\n```javascript\n// Example test structure\ndescribe('Placement Drives', () => {\n  test('scraper validates required fields', async () => {\n    const drive = { role: 'Developer' }; // Missing required fields\n    expect(scraper.validateDrive(drive)).toBe(false);\n  });\n  \n  test('eligibility matching works correctly', () => {\n    const user = { batch: '2024', cgpa: 7.5, branch: 'CSE' };\n    const drive = { batch: '2024', min_cgpa: 6.0, branches: ['CSE'] };\n    const result = eligibilityService.checkEligibility(user, drive);\n    expect(result.isEligible).toBe(true);\n  });\n});\n```\n\n## üîß Maintenance\n\n### Regular Tasks\n\n- **Weekly**: Review scraping success rates\n- **Monthly**: Update company URLs if needed\n- **Quarterly**: Analyze user engagement metrics\n\n### Monitoring Alerts\n\n- Scraping failures > 20%\n- Database connection issues\n- High notification counts (>1000 unread per user)\n- API response times > 2s\n\n## üö® Troubleshooting\n\n### Common Issues\n\n**Scraping Failures**\n```bash\n# Check logs\ntail -f logs/scraper.log\n\n# Test individual company\nnode -e \"require('./server/scrapers/accenture.scraper').scrape().then(console.log)\"\n```\n\n**Database Connection Issues**\n```bash\n# Test Supabase connection\nnode -e \"require('./server/config/supabaseClient').getClient().from('companies').select('*').then(console.log)\"\n```\n\n**Notification Issues**\n```javascript\n// Check notification service\nconst service = require('./server/services/notification.service');\nservice.getUnreadCount('user_id').then(console.log);\n```\n\n## üìö API Reference\n\n### Endpoints\n\n```typescript\n// Get placement drives\nGET /api/placement-drives?type=eligible&limit=20\n\n// Register for drive\nPOST /api/placement-drives\nBody: { action: 'register_drive', data: { driveId: 123 } }\n\n// Get notifications\nGET /api/notifications?unread=true&limit=10\n\n// Mark notifications as read\nPUT /api/notifications\nBody: { notificationId: 456 } | { markAllAsRead: true }\n\n// Update user profile\nPOST /api/user-profile\nBody: { batch: '2024', branch: 'CSE', cgpa: 7.5, college: 'ABC College' }\n```\n\n## üéâ Success Metrics\n\nTrack these KPIs to measure success:\n\n- **User Engagement**: Daily active users in placement drives section\n- **Registration Rate**: % of users who click \"Register Now\"\n- **Notification Effectiveness**: Open rate for notifications\n- **Data Accuracy**: % of drives with valid registration links\n- **System Reliability**: Scraper uptime and success rate\n\n---\n\n## ü§ù Contributing\n\nTo add new features or improve existing functionality:\n\n1. Follow the existing code patterns\n2. Add comprehensive error handling\n3. Include logging for debugging\n4. Test with multiple scenarios\n5. Update documentation\n\n## üìû Support\n\nFor issues or questions:\n- Check the troubleshooting section\n- Review scraper logs\n- Test individual components\n- Ensure environment variables are correct\n\n---\n\n**Happy Placement Drive Hunting! üöÄ**
